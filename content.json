{"pages":[{"title":"about","text":"hahha","link":"/about/index.html"}],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/02/02/hello-world/"},{"title":"Fast Massage Passing and Approximate Inference","text":"$$lim_{1\\to+\\infty}P(|\\frac{1}{n}\\sum_i^nX_i-\\mu|&lt;\\epsilon)=1, i=1,…,n$$ $ a+n_h=2 $$a+n_h=2$ $$ a+n_h=2 $$ sssss$a+n_j=2$ $a+b_n$ a+b_n sssssssssssssssssssssssssssssssssssssssssssss $$sqrt()$$ $$f([\\frac{1+{x,y}}{(\\frac{x}{y}+\\frac{y}{x})(u+1)}+a]^{3\\2})$$ $$f\\left(\\left[ \\frac{ 1+\\left{x,y\\right} }{ \\left( \\frac{x}{y}+\\frac{y}{x} \\right) \\left(u+1\\right) }+a\\right]^{3\\2}\\right)$$ MRF with Triple Penalty and Label Contexts过程和第二部分类似，只不过，Pairwise-Potential变成了Triple Penalty。即对于DPN，除了Label平滑度量变成了一个可学习的卷积核，还将$k(i,j)$变为了$k(i,z)p^v_z$。这样一来，在Mean-Field推断过程中， $$q_i(u) \\propto exp(-(\\psi_i+\\sum_{j \\in N_i} \\sum_{v}\\mu(i,u,j,v)\\sum_{z \\in N_j}k(j,z)q_z^v ))$$ 这个不同于以往的，相当于在i，j两个像素之间仅考虑了空间关系，没有考虑像素贴特征的相似度。即： $$q_i(u) \\propto exp(-(\\psi_i+\\sum_{j } \\sum_{v}\\mu(i,u,j,v)I(j \\in N_i)\\sum_{z \\in N_j}k(j,z)q_z^v ))$$ ！！！！怪不得DPN快呢，观察上面的式子，相当于在一次传播当中，完成了之前两次的Massage-Passing的内容哦。怪不得收敛快好多呢。 DPN引入了三元惩罚项以及空间标签平滑模块。性能提升了很多。其实，我有点怀疑性能的提升主要是来自于后者。换句话说，去掉三元惩罚项，取而代之的是ConvCRF，效果也是不错的。 可以说，在CRF推断当中，占据整体复杂度的Dominant的部分的就是Massege Passing。DenseCRF毫无疑问，可以很大程度上考虑到Long-Range Relationship，其模型复杂度也是很大的。$N^2$级别的Pairwise Potential在DenseCRF当中也成为了一个难以逾越的巨大难题。 DenseCRF的近似推断也变得特别麻烦。传统的基于采样的近似推断方法，例如MCMC，往往要推好几个小时。这样的复杂度是我们根本无法接受的。斯坦福的科学家提出了在CCCP框架下基于Mean-Field Approximation的推断方法，结合permutohedral-Lattice，极大地提升了推断效率。让万级别变量的CRF的推断速度从几个小时降低到了几秒钟。 但是，这个速度在讲求实时性的很多计算机视觉子领域同样是难以接受的。因为往往需要每秒钟处理好几祯的图像，但是CRF却成为了整个系统的瓶颈。例如对于DeepLab V1，如果不加入CRF作为后端，那么往往可以做到8FPS，但是加入了CRF之后，虽然mIoU数值上获得了提升，但是推理速度也降低到了2FPS。很多视觉分割问题，例如自动驾驶，直播视频处理等等，很难容忍这样低的处理速度。因此在最新的视觉分割工作当中，例如DeepLab v3、v3+等，彻底的抛弃了随机场模型。 深度学习可以很好的挖掘图像的语义属性，因此在图像识别，目标检测等领域获得了巨大的成功。然而，深度学习模型往往会忽略掉原像素空间的特征信息，而像素空间的特征关系往往能对语义分割问题有很好的性能改善，例如获得更清晰的边界信息。随机场模型可以很好的利用到像素特征，从而改善深度学习的分割性能。但是，正如上文所述，随机场模型的推断速度恰恰成为了一个难以跨过的鸿沟，限制了CRF的应用。 随机场模型的复杂度主要来自于Massage Passing步骤，很多工作对这一步骤进行了很多探索，例如基于permutohedral-Lattice的工作（CRFasRNN）。还有基于条件独立性假设的ConvCRF以及DPN（MMLab的这个工作比UCam的工作早两年）。条件独立性假设就是每一个像素点的Pairwise-Potential仅仅与其邻域内的像素点相作用，不再遵循DenseCRF当中全局连接的准则。再例如，沈春华课题组将Pairwise Potential变成了可学习的组件，整合到了深度网络当中。 对于标准的MF推断过程，如下所示： 初始化: $Q_i^{0}=\\frac{1}{z_i}exp(-\\psi_i)$ Do until converged for any label $v$and index $i$: Message-Passing: $Q_i^{(m)}(v)=\\sum_{j \\neq i}k^{(m)}_{ij}Q_j(v)$ Compatibility transform:$ Q_i(u)=\\sum_{v \\in L} \\mu^{(m)}(u,v)\\sum_mw^{(m)}Q_i^{(m))}(v)$ Local update: $Q_i=exp(-\\psi_i-Q_i)$ Normalize $Q_i$ 注意在CRF-RNN那篇文里面，人家没有推导错误，是$U_i=- \\psi(i)$。 这里首先进行Message-Passing。 对于ConvCRF而言，也是首先进行这一步骤，只是在条件独立假设下， $Q_i^{(m)}(l)=\\sum_{j \\in N_i}k^{(m)}_{ij}Q_j(l)$ 这里$N_i$表示变量（像素点）i的邻域。如果邻域是方形的，那么这一步的操作显然是一个类似卷积的操作，一个跨通道的卷积核在Probability-Map上进行滑动，只不过滑动方向不是x和y轴，而是在跨通道的同一个位置，（这里的通道数等于class数目）。这里绝大多数时间均消耗在了数据重组上面，适当的代码优化可以获得10-fold的速度提升。 初始化: $Q_i^{0}=\\frac{1}{z_i}exp(-\\psi_i)$ Do until converged for any label $v$and index $i$: Message-Passing: $Q_i^{(m)}(v)=\\sum_{j \\in N_i}k^{(m)}_{ij}Q_j(v)$ Compatibility transform:$ Q_i(u)=\\sum_{v \\in L} \\mu^{(m)}(u,v)\\sum_mw^{(m)}Q_i^{(m))}(v)$ Local update: $Q_i=exp(-\\psi_i-Q_i)$ Normalize $Q_i$ MRF with Pairwise-Penalty and Label Contexts 而对于DPN，这里就变得稍微复杂了一些。在考虑DPN之前，我们首先考虑一种特殊情况（中间过渡情况） 对于传统CRF，包括ConvCRF，他们定义的势能表达式都是： $$E(X|I)=\\sum_i \\psi_i +\\sum_{ij}\\phi_{ij}$$ $$= \\sum_i \\psi_i(x)+\\sum_{ij}\\mu(u,v)k(i,j)$$ 变分推断结果可以表示为： $$q_i(u) \\propto exp(-(\\psi_i+\\sum_{j \\neq i} \\sum_{v}\\phi_{i,j}^{u,v}q_j^v))$$ 对于DPN，MMLab将Label平滑度量变成了一个可学习的卷积核，认为Label存在空间可推理关系，变成了$\\mu(i,u,j,v)$的形式。也就是说，DPN默认势能表达形式为： $$E(X|I)=\\sum_i \\psi_i +\\sum_{i} \\sum_{j \\in N_i}\\phi_{ij}$$ $$= \\sum_i \\psi_i(x)+\\sum_{ij}\\mu(i,u,j,v)k(i,j)$$ 这样一来，在Mean-Field推断过程中， $$q_i(u) \\propto exp(-(\\psi_i+\\sum_{j \\in N_i} \\sum_{v}\\mu(i,u,j,v)k(i,j)q_j^v ))$$ 由于$\\mu(i,u,j,v)$已经不再和index独立，因此，推断步骤当中不能再将Compatibility transform和Message-Passing分开进行了。一共需要21*21个$\\mu$卷积核，每一个用于记录两个类别标签之间的潜在的空间可推理关系，例如人在摩托车上方，椅子不太可能出现在房顶上。 $\\mu $卷积核和k卷积核不同，u卷积核在一张特征图上共享即$\\mu(i,c)=\\mu(j,c)$。因此我们表示为$\\mu_{n,m}$，即当前第n个(class)通道对应是前一层第m个通道(class)的卷积核。 k卷积核则是跨特征图（通道）在同一个位置共享，即$k(i,c_1)=k(i,c_n)$，c是通道编号，i是位置编号。因此我们表示为k_{i}$，即当前第i个位置对应的卷积核。 在推断过程中，二者需要做一个点乘，构成一个新的k卷积核，k由于和$\\mu$点乘之后，不再是跨通道相等了。也就是说，假如输入特征图编号为m，我们在计算本层第n张图上第i个位置对应的卷积核为，$k_{n,m}(i)=k(i)* \\mu_{n,m}$ 初始化: $Q_i^{0}=\\frac{1}{z_i}exp(-\\psi_i)$ Do until converged for any label $v$and index $i$: Message-Passing and Compatibility transform:$ Q_i(\\mu)=\\sum_{m} w^{m} \\sum_{v \\in L} \\sum_{j \\in N_i} \\mu^{(m)}(i,u,j,v)k^{(m)}_{ij}Q_j(v)$ Local update: $Q_i=exp(-\\psi_i-Q_i)$ Normalize $Q_i$ DPN文中将$w^m$的变成了跨通道max-pooling。 MRF with Triple Penalty and Label Contexts 过程和上述类似，只不过，Pairwise-Potential变成了Triple Penalty。即对于DPN，除了Label平滑度量变成了一个可学习的卷积核，还将$k(i,j)$变为了$k(i,z)p^v_z$。这样一来，在Mean-Field推断过程中， 1q_i(u) \\propto exp(-(\\psi_i+\\sum_{j \\in N_i} \\sum_{v}\\mu(i,u,j,v)\\sum_{z \\in N_j}k(j,z)q_z^v )) 这个不同于以往的，相当于在i，j两个像素之间仅考虑了空间关系，没有考虑像素贴特征的相似度。即：12q_i(u) \\propto exp(-(\\psi_i+\\sum_{j } \\sum_{v}\\mu(i,u,j,v)I(j \\in N_i)\\sum_{z \\in N_j}k(j,z)q_z^v )) ！！！！怪不得DPN快呢，观察上面的式子，相当于在一次传播当中，完成了之前两次的Massage-Passing的内容哦。怪不得收敛快好多呢。 $$\\begin{aligned}f_Y(y) &amp; = f_X[h(y)]|h’(y)| \\[2ex]&amp; = f_X[h(y)]h’(y) \\[2ex]&amp; = \\frac{1}{\\theta}e^{-\\frac{x}{\\theta}}[\\frac{dx}{dy}(-\\frac{\\theta}{ln(1-y)})] \\[2ex]&amp; = \\frac{1}{\\theta}e^{-\\frac{-\\frac{\\theta}{ln(1-y)}}{\\theta}}\\frac{\\theta}{1-y} \\[2ex]&amp; = \\frac{1}{\\theta}e^{ln(1-y)}\\frac{\\theta}{1-y} \\[2ex]&amp; = \\frac{1-y}{\\theta}\\frac{\\theta}{1-y} \\[2ex]&amp; = 1\\end{aligned}$$ fuck dddddddddddddd$$\\begin{eqnarray}\\nabla\\cdot\\vec{E} &amp;=&amp; \\frac{\\rho}{\\epsilon_0} \\\\nabla\\cdot\\vec{B} &amp;=&amp; 0 \\\\nabla\\times\\vec{E} &amp;=&amp; -\\frac{\\partial B}{\\partial t} \\\\nabla\\times\\vec{B} &amp;=&amp; \\mu_0\\left(\\vec{J}+\\epsilon_0\\frac{\\partial E}{\\partial t} \\right)\\end{eqnarray}$$ Deep Massage Passing Nets很多东西总结的很到位，例如：传统方法都是基于learned-Potential+迭代推断，而利用一个优化后的网络直接去学习这种转播当中的massage可以摆脱上述框架。事实证明，效果上还是不错的。CNN不再是产生rough-prediction，而是产生概率图中每个变量节点所对应的特征向量。近似推断所需要的多次迭代则被参数化到了另一个网络当中。直接预测Massage不是沈春华课题组的原创，之前CMU-IR尝试过直接预测Variable2Factor，这个貌似需要的输出节点非常多？ 直接预测Potential是需要$K^a$的输出个数的。而Variable2Factor则需要计算这个potential，直接导致参数量激增。不太划算，例如pairwise-potential需要$K^2$的输出，还记得U矩阵吗？","link":"/2019/02/02/a/"}],"tags":[{"name":"Massage-Passing","slug":"Massage-Passing","link":"/tags/Massage-Passing/"}],"categories":[]}